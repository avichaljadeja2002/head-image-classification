{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "Epoch 1/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 718ms/step - classification_accuracy: 0.2450 - loss: 0.8248 - val_classification_accuracy: 0.3333 - val_loss: 0.8003\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348ms/step - classification_accuracy: 0.2654 - loss: 0.7905 - val_classification_accuracy: 0.1944 - val_loss: 0.8097\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step - classification_accuracy: 0.2752 - loss: 0.7735 - val_classification_accuracy: 0.1944 - val_loss: 0.8147\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397ms/step - classification_accuracy: 0.2556 - loss: 0.7796 - val_classification_accuracy: 0.1944 - val_loss: 0.8051\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382ms/step - classification_accuracy: 0.2732 - loss: 0.7715 - val_classification_accuracy: 0.1944 - val_loss: 0.7916\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394ms/step - classification_accuracy: 0.2928 - loss: 0.7592 - val_classification_accuracy: 0.1944 - val_loss: 0.7629\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394ms/step - classification_accuracy: 0.2771 - loss: 0.7426 - val_classification_accuracy: 0.1944 - val_loss: 0.7380\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380ms/step - classification_accuracy: 0.2654 - loss: 0.7233 - val_classification_accuracy: 0.1944 - val_loss: 0.7227\n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374ms/step - classification_accuracy: 0.2888 - loss: 0.7110 - val_classification_accuracy: 0.1944 - val_loss: 0.7169\n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 400ms/step - classification_accuracy: 0.2752 - loss: 0.7072 - val_classification_accuracy: 0.1944 - val_loss: 0.7155\n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403ms/step - classification_accuracy: 0.2869 - loss: 0.7056 - val_classification_accuracy: 0.1944 - val_loss: 0.7142\n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360ms/step - classification_accuracy: 0.3123 - loss: 0.7036 - val_classification_accuracy: 0.1944 - val_loss: 0.7125\n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355ms/step - classification_accuracy: 0.3025 - loss: 0.7008 - val_classification_accuracy: 0.1944 - val_loss: 0.7082\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377ms/step - classification_accuracy: 0.2678 - loss: 0.7023 - val_classification_accuracy: 0.1944 - val_loss: 0.7049\n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 365ms/step - classification_accuracy: 0.2810 - loss: 0.6996 - val_classification_accuracy: 0.1944 - val_loss: 0.7052\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377ms/step - classification_accuracy: 0.2908 - loss: 0.6994 - val_classification_accuracy: 0.1944 - val_loss: 0.7044\n",
      "Epoch 17/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401ms/step - classification_accuracy: 0.3016 - loss: 0.6970 - val_classification_accuracy: 0.1944 - val_loss: 0.7049\n",
      "Epoch 18/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 474ms/step - classification_accuracy: 0.3212 - loss: 0.6956 - val_classification_accuracy: 0.1944 - val_loss: 0.7041\n",
      "Epoch 19/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 486ms/step - classification_accuracy: 0.2977 - loss: 0.6946 - val_classification_accuracy: 0.1944 - val_loss: 0.7011\n",
      "Epoch 20/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445ms/step - classification_accuracy: 0.3095 - loss: 0.6939 - val_classification_accuracy: 0.1944 - val_loss: 0.6998\n",
      "Epoch 21/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 445ms/step - classification_accuracy: 0.3110 - loss: 0.6937 - val_classification_accuracy: 0.2500 - val_loss: 0.6988\n",
      "Epoch 22/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373ms/step - classification_accuracy: 0.3583 - loss: 0.6934 - val_classification_accuracy: 0.1944 - val_loss: 0.6977\n",
      "Epoch 23/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374ms/step - classification_accuracy: 0.3336 - loss: 0.6919 - val_classification_accuracy: 0.2500 - val_loss: 0.6975\n",
      "Epoch 24/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 367ms/step - classification_accuracy: 0.4201 - loss: 0.6904 - val_classification_accuracy: 0.2500 - val_loss: 0.6962\n",
      "Epoch 25/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364ms/step - classification_accuracy: 0.2726 - loss: 0.6911 - val_classification_accuracy: 0.3611 - val_loss: 0.6935\n",
      "Epoch 26/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403ms/step - classification_accuracy: 0.2667 - loss: 0.6910 - val_classification_accuracy: 0.3889 - val_loss: 0.6918\n",
      "Epoch 27/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372ms/step - classification_accuracy: 0.4525 - loss: 0.6888 - val_classification_accuracy: 0.4167 - val_loss: 0.6922\n",
      "Epoch 28/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370ms/step - classification_accuracy: 0.4874 - loss: 0.6868 - val_classification_accuracy: 0.2778 - val_loss: 0.6914\n",
      "Epoch 29/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 350ms/step - classification_accuracy: 0.3301 - loss: 0.6873 - val_classification_accuracy: 0.3889 - val_loss: 0.6899\n",
      "Epoch 30/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364ms/step - classification_accuracy: 0.4052 - loss: 0.6851 - val_classification_accuracy: 0.3611 - val_loss: 0.6881\n",
      "Epoch 31/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373ms/step - classification_accuracy: 0.4588 - loss: 0.6827 - val_classification_accuracy: 0.2778 - val_loss: 0.6875\n",
      "Epoch 32/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372ms/step - classification_accuracy: 0.4314 - loss: 0.6818 - val_classification_accuracy: 0.2222 - val_loss: 0.6904\n",
      "Epoch 33/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371ms/step - classification_accuracy: 0.3598 - loss: 0.6800 - val_classification_accuracy: 0.3056 - val_loss: 0.6843\n",
      "Epoch 34/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 535ms/step - classification_accuracy: 0.4883 - loss: 0.6768 - val_classification_accuracy: 0.4167 - val_loss: 0.6770\n",
      "Epoch 35/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 448ms/step - classification_accuracy: 0.3583 - loss: 0.6745 - val_classification_accuracy: 0.5278 - val_loss: 0.6723\n",
      "Epoch 36/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382ms/step - classification_accuracy: 0.4034 - loss: 0.6754 - val_classification_accuracy: 0.3889 - val_loss: 0.6853\n",
      "Epoch 37/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373ms/step - classification_accuracy: 0.3568 - loss: 0.6803 - val_classification_accuracy: 0.3889 - val_loss: 0.6713\n",
      "Epoch 38/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 365ms/step - classification_accuracy: 0.3179 - loss: 0.6701 - val_classification_accuracy: 0.5278 - val_loss: 0.6730\n",
      "Epoch 39/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372ms/step - classification_accuracy: 0.4798 - loss: 0.6697 - val_classification_accuracy: 0.5000 - val_loss: 0.6688\n",
      "Epoch 40/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378ms/step - classification_accuracy: 0.4601 - loss: 0.6631 - val_classification_accuracy: 0.3333 - val_loss: 0.6705\n",
      "Epoch 41/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384ms/step - classification_accuracy: 0.3455 - loss: 0.6613 - val_classification_accuracy: 0.2500 - val_loss: 0.6893\n",
      "Epoch 42/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360ms/step - classification_accuracy: 0.4080 - loss: 0.6652 - val_classification_accuracy: 0.3333 - val_loss: 0.6843\n",
      "Epoch 43/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step - classification_accuracy: 0.4640 - loss: 0.6563 - val_classification_accuracy: 0.4167 - val_loss: 0.6610\n",
      "Epoch 44/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 366ms/step - classification_accuracy: 0.5788 - loss: 0.6489 - val_classification_accuracy: 0.4722 - val_loss: 0.6480\n",
      "Epoch 45/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 363ms/step - classification_accuracy: 0.4047 - loss: 0.6516 - val_classification_accuracy: 0.3889 - val_loss: 0.6457\n",
      "Epoch 46/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351ms/step - classification_accuracy: 0.3878 - loss: 0.6458 - val_classification_accuracy: 0.4722 - val_loss: 0.6442\n",
      "Epoch 47/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341ms/step - classification_accuracy: 0.4247 - loss: 0.6380 - val_classification_accuracy: 0.4167 - val_loss: 0.6550\n",
      "Epoch 48/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397ms/step - classification_accuracy: 0.3557 - loss: 0.6385 - val_classification_accuracy: 0.4722 - val_loss: 0.6626\n",
      "Epoch 49/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361ms/step - classification_accuracy: 0.5467 - loss: 0.6454 - val_classification_accuracy: 0.3056 - val_loss: 0.6792\n",
      "Epoch 50/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372ms/step - classification_accuracy: 0.3882 - loss: 0.6407 - val_classification_accuracy: 0.3056 - val_loss: 0.6654\n",
      "Epoch 51/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390ms/step - classification_accuracy: 0.4319 - loss: 0.6389 - val_classification_accuracy: 0.6667 - val_loss: 0.6323\n",
      "Epoch 52/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 362ms/step - classification_accuracy: 0.7476 - loss: 0.6225 - val_classification_accuracy: 0.6389 - val_loss: 0.6198\n",
      "Epoch 53/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407ms/step - classification_accuracy: 0.6050 - loss: 0.6146 - val_classification_accuracy: 0.5556 - val_loss: 0.6103\n",
      "Epoch 54/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 448ms/step - classification_accuracy: 0.4954 - loss: 0.6082 - val_classification_accuracy: 0.6389 - val_loss: 0.6032\n",
      "Epoch 55/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 424ms/step - classification_accuracy: 0.6897 - loss: 0.5977 - val_classification_accuracy: 0.3056 - val_loss: 0.6292\n",
      "Epoch 56/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 432ms/step - classification_accuracy: 0.4295 - loss: 0.5991 - val_classification_accuracy: 0.6944 - val_loss: 0.5989\n",
      "Epoch 57/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461ms/step - classification_accuracy: 0.7214 - loss: 0.5823 - val_classification_accuracy: 0.7500 - val_loss: 0.5883\n",
      "Epoch 58/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 432ms/step - classification_accuracy: 0.6461 - loss: 0.5756 - val_classification_accuracy: 0.4722 - val_loss: 0.5924\n",
      "Epoch 59/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 429ms/step - classification_accuracy: 0.6443 - loss: 0.5747 - val_classification_accuracy: 0.6389 - val_loss: 0.5857\n",
      "Epoch 60/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381ms/step - classification_accuracy: 0.6801 - loss: 0.5668 - val_classification_accuracy: 0.4722 - val_loss: 0.5811\n",
      "Epoch 61/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 671ms/step - classification_accuracy: 0.6523 - loss: 0.5477 - val_classification_accuracy: 0.6667 - val_loss: 0.5477\n",
      "Epoch 62/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409ms/step - classification_accuracy: 0.7928 - loss: 0.5331 - val_classification_accuracy: 0.6389 - val_loss: 0.5460\n",
      "Epoch 63/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 443ms/step - classification_accuracy: 0.7719 - loss: 0.5206 - val_classification_accuracy: 0.6944 - val_loss: 0.5330\n",
      "Epoch 64/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397ms/step - classification_accuracy: 0.8184 - loss: 0.5113 - val_classification_accuracy: 0.5556 - val_loss: 0.5419\n",
      "Epoch 65/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383ms/step - classification_accuracy: 0.6539 - loss: 0.5147 - val_classification_accuracy: 0.6667 - val_loss: 0.5139\n",
      "Epoch 66/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395ms/step - classification_accuracy: 0.6938 - loss: 0.4916 - val_classification_accuracy: 0.6944 - val_loss: 0.5026\n",
      "Epoch 67/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380ms/step - classification_accuracy: 0.7394 - loss: 0.4861 - val_classification_accuracy: 0.8056 - val_loss: 0.5014\n",
      "Epoch 68/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390ms/step - classification_accuracy: 0.7552 - loss: 0.4680 - val_classification_accuracy: 0.6389 - val_loss: 0.4871\n",
      "Epoch 69/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 367ms/step - classification_accuracy: 0.6139 - loss: 0.4801 - val_classification_accuracy: 0.8333 - val_loss: 0.4548\n",
      "Epoch 70/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371ms/step - classification_accuracy: 0.8316 - loss: 0.4354 - val_classification_accuracy: 0.4167 - val_loss: 0.4914\n",
      "Epoch 71/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395ms/step - classification_accuracy: 0.5655 - loss: 0.4726 - val_classification_accuracy: 0.7778 - val_loss: 0.4360\n",
      "Epoch 72/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 405ms/step - classification_accuracy: 0.7901 - loss: 0.4207 - val_classification_accuracy: 0.8611 - val_loss: 0.4066\n",
      "Epoch 73/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 519ms/step - classification_accuracy: 0.9041 - loss: 0.4059 - val_classification_accuracy: 0.6667 - val_loss: 0.4535\n",
      "Epoch 74/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 345ms/step - classification_accuracy: 0.7728 - loss: 0.4262 - val_classification_accuracy: 0.7222 - val_loss: 0.4039\n",
      "Epoch 75/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445ms/step - classification_accuracy: 0.8214 - loss: 0.3897 - val_classification_accuracy: 0.6111 - val_loss: 0.4521\n",
      "Epoch 76/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380ms/step - classification_accuracy: 0.7027 - loss: 0.4097 - val_classification_accuracy: 0.6389 - val_loss: 0.4477\n",
      "Epoch 77/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354ms/step - classification_accuracy: 0.6257 - loss: 0.4452 - val_classification_accuracy: 0.8056 - val_loss: 0.3880\n",
      "Epoch 78/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 367ms/step - classification_accuracy: 0.8937 - loss: 0.3651 - val_classification_accuracy: 0.8333 - val_loss: 0.3623\n",
      "Epoch 79/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356ms/step - classification_accuracy: 0.8533 - loss: 0.3585 - val_classification_accuracy: 0.8333 - val_loss: 0.3397\n",
      "Epoch 80/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step - classification_accuracy: 0.8607 - loss: 0.3404 - val_classification_accuracy: 0.8889 - val_loss: 0.3488\n",
      "Epoch 81/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 347ms/step - classification_accuracy: 0.8685 - loss: 0.3403 - val_classification_accuracy: 0.8889 - val_loss: 0.3197\n",
      "Epoch 82/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356ms/step - classification_accuracy: 0.8665 - loss: 0.3168 - val_classification_accuracy: 0.8056 - val_loss: 0.3378\n",
      "Epoch 83/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357ms/step - classification_accuracy: 0.8342 - loss: 0.3208 - val_classification_accuracy: 0.8889 - val_loss: 0.2791\n",
      "Epoch 84/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 353ms/step - classification_accuracy: 0.7726 - loss: 0.3079 - val_classification_accuracy: 0.9444 - val_loss: 0.2819\n",
      "Epoch 85/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 366ms/step - classification_accuracy: 0.9334 - loss: 0.2759 - val_classification_accuracy: 0.8333 - val_loss: 0.2823\n",
      "Epoch 86/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 365ms/step - classification_accuracy: 0.8811 - loss: 0.2694 - val_classification_accuracy: 0.9167 - val_loss: 0.2610\n",
      "Epoch 87/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355ms/step - classification_accuracy: 0.9386 - loss: 0.2621 - val_classification_accuracy: 0.8056 - val_loss: 0.2847\n",
      "Epoch 88/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359ms/step - classification_accuracy: 0.8691 - loss: 0.2520 - val_classification_accuracy: 0.9444 - val_loss: 0.2412\n",
      "Epoch 89/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382ms/step - classification_accuracy: 0.9538 - loss: 0.2397 - val_classification_accuracy: 0.9444 - val_loss: 0.2502\n",
      "Epoch 90/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 352ms/step - classification_accuracy: 0.9089 - loss: 0.2358 - val_classification_accuracy: 0.9167 - val_loss: 0.2230\n",
      "Epoch 91/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373ms/step - classification_accuracy: 0.9010 - loss: 0.2368 - val_classification_accuracy: 0.7500 - val_loss: 0.2853\n",
      "Epoch 92/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359ms/step - classification_accuracy: 0.8279 - loss: 0.2405 - val_classification_accuracy: 0.9444 - val_loss: 0.2200\n",
      "Epoch 93/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378ms/step - classification_accuracy: 0.9440 - loss: 0.2365 - val_classification_accuracy: 0.8889 - val_loss: 0.2204\n",
      "Epoch 94/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346ms/step - classification_accuracy: 0.8809 - loss: 0.2094 - val_classification_accuracy: 0.9167 - val_loss: 0.2235\n",
      "Epoch 95/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408ms/step - classification_accuracy: 1.0000 - loss: 0.1982 - val_classification_accuracy: 0.7500 - val_loss: 0.3227\n",
      "Epoch 96/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 411ms/step - classification_accuracy: 0.8624 - loss: 0.2559 - val_classification_accuracy: 0.8611 - val_loss: 0.2076\n",
      "Epoch 97/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 367ms/step - classification_accuracy: 0.9049 - loss: 0.2043 - val_classification_accuracy: 0.9444 - val_loss: 0.1695\n",
      "Epoch 98/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389ms/step - classification_accuracy: 0.9787 - loss: 0.1648 - val_classification_accuracy: 0.8889 - val_loss: 0.1814\n",
      "Epoch 99/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 347ms/step - classification_accuracy: 0.9227 - loss: 0.1781 - val_classification_accuracy: 0.8333 - val_loss: 0.1825\n",
      "Epoch 100/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373ms/step - classification_accuracy: 0.8889 - loss: 0.1658 - val_classification_accuracy: 1.0000 - val_loss: 0.1429\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 312ms/step\n",
      "Actual: left, Predicted: left\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: up, Predicted: up\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: left, Predicted: left\n",
      "Actual: right, Predicted: right\n",
      "Actual: up, Predicted: up\n",
      "Actual: right, Predicted: right\n",
      "Actual: right, Predicted: right\n",
      "Actual: left, Predicted: left\n",
      "Actual: up, Predicted: up\n",
      "Actual: up, Predicted: up\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: left, Predicted: left\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: right, Predicted: right\n",
      "Actual: left, Predicted: left\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: left, Predicted: left\n",
      "Actual: right, Predicted: right\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: right, Predicted: right\n",
      "Actual: left, Predicted: left\n",
      "Actual: up, Predicted: up\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: left, Predicted: left\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: up, Predicted: up\n",
      "Actual: right, Predicted: right\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: straight, Predicted: straight\n",
      "Actual: right, Predicted: right\n",
      "Actual: up, Predicted: up\n",
      "Actual: right, Predicted: right\n",
      "Total Correct: 36\n",
      "Total Wrong: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, GlobalAveragePooling2D, Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras\n",
    "\n",
    "def load_images_with_labels(base_directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    processed_counts = {}\n",
    "    label_map = {'up': 0, 'straight': 1, 'left': 2, 'right': 3}\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        folder_name = os.path.basename(root)\n",
    "        processed_counts[folder_name] = {'total': 0, 'processed': 0, 'skipped': 0}\n",
    "        \n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.pgm')):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    processed_counts[folder_name]['total'] += 1\n",
    "                    \n",
    "                    img = Image.open(file_path).convert('RGB')\n",
    "                    img = img.resize((64, 64))\n",
    "                    img_array = np.array(img) / 255.0\n",
    "                    \n",
    "                    parts = filename.split('_')\n",
    "                    if len(parts) > 1:\n",
    "                        orientation = parts[1]\n",
    "                        if orientation in label_map:\n",
    "                            images.append(img_array)\n",
    "                            labels.append(label_map[orientation])\n",
    "                            processed_counts[folder_name]['processed'] += 1\n",
    "                        else:\n",
    "                            processed_counts[folder_name]['skipped'] += 1\n",
    "                            print(f\"Skipping file with unexpected orientation: {filename} in {folder_name}\")\n",
    "                    else:\n",
    "                        processed_counts[folder_name]['skipped'] += 1\n",
    "                        print(f\"Skipping file with missing orientation: {filename} in {folder_name}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    processed_counts[folder_name]['skipped'] += 1\n",
    "                    print(f\"Error processing {filename} in {folder_name}: {e}\")\n",
    "    \n",
    "    # # Print processing summary\n",
    "    # print(\"\\nProcessing Summary:\")\n",
    "    # for folder, counts in processed_counts.items():\n",
    "    #     if counts['total'] > 0: \n",
    "    #         print(f\"\\n{folder}:\")\n",
    "    #         print(f\"  Total files: {counts['total']}\")\n",
    "    #         print(f\"  Successfully processed: {counts['processed']}\")\n",
    "    #         print(f\"  Skipped: {counts['skipped']}\")\n",
    "    #         if counts['processed'] > 0:\n",
    "    #             success_rate = (counts['processed'] / counts['total']) * 100\n",
    "    #             print(f\"  Success rate: {success_rate:.2f}%\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def create_combined_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = UpSampling2D((2, 2))(encoded)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same', name='decoded')(x)\n",
    "\n",
    "    # Classification head\n",
    "    x = GlobalAveragePooling2D()(encoded)\n",
    "    classification_output = Dense(num_classes, activation='softmax', name='classification')(x)\n",
    "\n",
    "    model = Model(inputs, [decoded, classification_output])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={\n",
    "            'decoded': 'mean_squared_error',\n",
    "            'classification': 'categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'decoded': 1.0,\n",
    "            'classification': 0.5\n",
    "        },\n",
    "        metrics={\n",
    "            'classification': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "directory = 'faces/tammo'\n",
    "images, labels = load_images_with_labels(directory)\n",
    "print(len(images))\n",
    "if len(images) == 0 or len(labels) == 0:\n",
    "    print(\"No data found. Ensure the directory is correct and files match the expected format.\")\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "    y_train_cat = to_categorical(y_train, num_classes=4)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=4)\n",
    "\n",
    "    combined_model = create_combined_model(input_shape=(64, 64, 3), num_classes=4)\n",
    "\n",
    "    combined_model.fit(\n",
    "        X_train,\n",
    "        {\n",
    "            'decoded': X_train,\n",
    "            'classification': y_train_cat \n",
    "        },\n",
    "        batch_size=64,\n",
    "        epochs=100,\n",
    "        validation_data=(\n",
    "            X_test,\n",
    "            {\n",
    "                'decoded': X_test,\n",
    "                'classification': y_test_cat\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    decoded_imgs, predictions = combined_model.predict(X_test)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    orientation_map = {0: 'up', 1: 'straight', 2: 'left', 3: 'right'}\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    for i in range(len(y_test)):\n",
    "        actual = orientation_map[y_test[i]]\n",
    "        predicted = orientation_map[predicted_labels[i]]\n",
    "        print(f\"Actual: {actual}, Predicted: {predicted}\")\n",
    "        if(actual == predicted):\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    print(f\"Total Correct: {correct}\")\n",
    "    print(f\"Total Wrong: {wrong}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
